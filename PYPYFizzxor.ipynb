{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled68.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPknZVkkCW0sGbUjwGT+Srh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanksghub/Python-Practice-July/blob/main/PYPYFizzxor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa85LrjtRxSV"
      },
      "source": [
        "\"\"\" A tnsor is an n-dimnesional array\"\"\"\n",
        "\n",
        "from numpy import ndarray as Tensor\n",
        "from typing import Sequence, Iterator, Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlh-Gs1eil16"
      },
      "source": [
        "\"\"\" a Loss function measures how good our\n",
        " predictions are, we can use this to adjust \n",
        " the parameters of our networks. \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from shanksnet.tensor import Tensor\n",
        "\n",
        "class MSE(Loss):\n",
        "  \"\"\"MSE is mean square error\"\"\"\n",
        "  def loss(self, predicted:Tensor, actual:Tensor)->float:\n",
        "      return np.sum((predicted- actual)**2)\n",
        "\n",
        "  def grad(self, predicted: TEnsor, actual :tensor) -> tensor:\n",
        "      return 2* (predicted - actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "2NO44d1JkinO",
        "outputId": "590e2971-d00f-45d2-dd16-9c4b5d40a7fb"
      },
      "source": [
        "\"\"\" Neural Nets are made of layers, every klayer need to pass its\n",
        " imputs forward and propagate grdients backwards.\"\"\"\n",
        "\n",
        "from shanksnet.tensor import tensor\n",
        "import numpy as np\n",
        "from typing import Dict, Callable\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self)-> None:\n",
        "    self.params: Dict[str, Tensor] = {}\n",
        "    self.grads: Dict[str, Tensor] = {}\n",
        "\n",
        "  def forward(self, inputs:Tensor)-> Tensor:\n",
        "     \"\"\" produce the outputs corresponding to these inputs\"\"\"\n",
        "     raise NotImplementedError\n",
        "\n",
        "  def backward(self, grad: Tensor)-> Tensor:\n",
        "    \"\"\" backpropagate this gradient through the layer\"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "\n",
        "class Linear(layer):\n",
        "  \"\"\"computes output = inputs @ w + b\"\"\"\n",
        "  super.__init__()\n",
        "  def __init__(self, input_size: int, output_size:int)-> None:\n",
        "    # inputs will be (batch_size, input_size)\n",
        "    # outputs will be (batch_size, input_size)\n",
        "\n",
        "    self.params[\"w\"] = np.random.randn(input_size, output_size)\n",
        "\n",
        "    self.params[\"b\"] = np.random.randn(output_size)\n",
        "\n",
        "\n",
        "  def forward(self, input: Tensor):\n",
        "    \"\"\"outputs = inputs @ w + b\"\"\"\n",
        "\n",
        "    self.inputs = inputs\n",
        "    return inputs @ self.params[\"w\"] + self.params[\"b\"] \n",
        "\n",
        "  def backward(self, grad: Tensor)-> Tensor:\n",
        "    \"\"\" if y = f(x) and x = a * b + c \n",
        "    then dy/da = f'(x)*b\n",
        "    and dy/db = f'(x) * a\n",
        "    and dy/dc = f'(x)\n",
        "\n",
        "    if y = f(x) and x = a @ b + c\n",
        "    then dy/da = f'(x) @ b.T\n",
        "    and dy/db = a.T @ f'(x)\n",
        "    and dy/dc = f'(x)\n",
        "    \"\"\"\n",
        "\n",
        "    self.grads[\"b\"] = np.sum(grad, axis= 0)\n",
        "    self.grads[\"w\"] = self.inputs.T @ grad\n",
        "\n",
        "    return grad @ self.params[\"w\"].T\n",
        "\n",
        "F = Callable[[Tensor], tensor]\n",
        "class Activation(layer):\n",
        "  \"\"\" An activation layer just applies a function\n",
        "   elementwise to its inputs\"\"\"\n",
        "\n",
        "    def __init__(self, f:F, f_prime:F)-> None:\n",
        "      super().__init__()\n",
        "      self.f = f\n",
        "      self.f_prime = f_prime\n",
        "\n",
        "    def forward(self, inputs: Tensor)-> Tensor:\n",
        "        self.inputs =  inputs\n",
        "        return self.f(inputs)\n",
        "\n",
        "\n",
        "    def backward(self, grad: Tensor)--> Tensor:\n",
        "        \n",
        "        return self.f_prime(self.inputs) * grads    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tanh(x: Tensor) -> Tensor:\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x: Tensor) -> Tensor:\n",
        "    y = tanh(x)\n",
        "    return 1 - y ** 2\n",
        "\n",
        "\n",
        "class Tanh(Activation):\n",
        "  def __init__(self):\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-aa84634d9e46>\"\u001b[0;36m, line \u001b[0;32m62\u001b[0m\n\u001b[0;31m    def __init__(self, f:F,F_prime:F)-> None:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbMzabDfGOSQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMXM8NjeGOVE"
      },
      "source": [
        "\"\"\" A Neural Net is just  collection of layers. \"\"\"\n",
        "\n",
        "from shanksnet.layers import Layer\n",
        "from shanksnet.tensor import Tensor\n",
        "\n",
        "class NeuralNet:\n",
        "    def __init__(self, layers: Sequence[Layer])-> None:\n",
        "        self.layers = layers\n",
        "\n",
        "    def forward(self, inputs:Tensor)-> Tensor:\n",
        "        for layer in self.layers:\n",
        "          inputs = layer.forward(inputs)\n",
        "        return inputs\n",
        "\n",
        "    def backward(self, grad: Tensor)-> Tensor:\n",
        "      for layer in reversed(self.layers):\n",
        "        grad = layer.backward(grad)\n",
        "      return grad        \n",
        "\n",
        "def params_and_grads(self) - > Iterator[Tuple[Tensor,Tensor]]:\n",
        "  for layer in self.layers:\n",
        "      for name, param in layers.params.items()\n",
        "          grad = layer.grads[name]\n",
        "          yield param, grad   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um_047axNEs_"
      },
      "source": [
        "\"\"\" We use an optimizer to adjust the\n",
        " parameters of our network based on the\n",
        "  gradients computed during backpropagation\"\"\"\n",
        "from shanksnet.nn import Neuralnet\n",
        "\n",
        "class\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXV3PQAIYSdu"
      },
      "source": [
        "\"\"\" we will feed inputs into our networks in batcehs.. here are some tools\"\"\"\n",
        "\n",
        "\n",
        "from typing import Iterator, namedTuple\n",
        "from shanksnet.tensor import tensor\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "Batch = NamedTuple(\"Batch\",[(\"inputs\", Tensor),(\"targets\",Tensor)])\n",
        "\n",
        "\n",
        "class DataIterator:\n",
        "  def __call__(self, inputs: Tensor, targets: Tensor)-> Iterator\n",
        "      raise NotImplementedError\n",
        "\n",
        "\n",
        "class BatchIterator(DataIterator):\n",
        "  def __init__(self, batch_size:int = 32, shuffle: bool= True) -> None:\n",
        "    self.batch_size = batch_size\n",
        "    self.shuffle = shuffle\n",
        "\n",
        "  def __call__(self, inputs: Tensor, targets:Tensor)-> Iterator\n",
        "      starts = np.arange0,len(inputs,self.batch_size)\n",
        "      if self.shuffle:\n",
        "          np.random.shuffle(starts)    \n",
        "\n",
        "      for start in starts:\n",
        "          end = start + self.batch_size\n",
        "          batch_inputs = inputs[start:end]\n",
        "          batch_targets = targets[start:end]\n",
        "          yield Batch(batch_inputs, batch_targets)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRRkTgO3aO9B"
      },
      "source": [
        "\"\"\" function for training a neural net\"\"\"\n",
        "\n",
        "from shanksnet.tensor import Tensor\n",
        "from shanksnet.nn import NeuralNet\n",
        "from shanksnet.loss import Loss, MSE\n",
        "from shanksnet.optim import Optimizer, SGD\n",
        "from shanksnet.data import DataIterator, BatchIterator\n",
        "\n",
        "def train(net: Neuralnet, inputs: Tensor, \\\n",
        "          targets: Tensor, num_epochs:int=4000,\n",
        "          iterator: DataIterator = BatchIterator()\n",
        "          loss:loss = MSE()\n",
        "          optimizer: optimizer=SGD()) -> None:\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        for batch in iterator(inputs, targets):\n",
        "            predicted = net.forward(batch.inputs)\n",
        "            epoch_loss = loss.loss(predicted,batch.targets) \n",
        "            grad = loss.grad(predicted, batch.targets)\n",
        "            net.backward(grad)\n",
        "            optimizer.step(net)\n",
        "        print(epoch, epoch_loss)             \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKCnTHZ6cEyJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from shanksnet.train import train\n",
        "from shanksnet.nn import NeuralNet\n",
        "from shanksnet.layers import Linear, tanh\n",
        "\n",
        "inputs = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
        "\n",
        "targets = np.array([[1,0],[0,1],[0,1],[1,0]])\n",
        "\n",
        "net = neuralNet([Linear(input_size=2, output_size=2)])\n",
        "\n",
        "train(net, inputs, targets)\n",
        "\n",
        "for x, y in zip(inputs, targets):\n",
        "    predicted = net.forward(x)\n",
        "    print(x, predicted, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHkeOEz-b8xI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rjc7wOVjb2gR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}