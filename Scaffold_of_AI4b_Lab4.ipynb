{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaffold_of_AI4b_Lab4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanksghub/Python-Practice-July/blob/main/Scaffold_of_AI4b_Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLd7kZu32Nk5"
      },
      "source": [
        "# Mode Collapse and WGANs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltC6Gxz6qB3u"
      },
      "source": [
        "The aim of this lab is to:\n",
        "\n",
        "\n",
        "1.   See mode collapse in action on a synthetic dataset\n",
        "2.   Implement a WGAN on the synthetic dataset to see what changes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8L8kJhUPgMk"
      },
      "source": [
        "ss = (9,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljDxub-FPhsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cc6ca3-a13f-4862-80e2-1cbc387ce2e9"
      },
      "source": [
        "ss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoBsX4ZO2SPV"
      },
      "source": [
        "### 1. Mode Collapse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-klf39ms3NF"
      },
      "source": [
        "First we build a vanilla GAN on the the following synthetic dataset which has 4 modes.\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/dPbCXHW/multimodal-gaussianmix.png\" width=\"400\" />\n",
        "<figcaption>\n",
        "</div>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHJZvV9fPIwE"
      },
      "source": [
        "sdfs =(3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_PHBiq2cJtG"
      },
      "source": [
        "Here is the architecture we'll be using \n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://i.ibb.co/fYzWhGc/lab4-arch.jpg\" width=\"600\" />\n",
        "</center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giyi6VBPnaGa"
      },
      "source": [
        "## Vanilla GAN on synthetic data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi5B0pkQ2KPd"
      },
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from IPython.display import HTML\n",
        "from matplotlib.animation import FuncAnimation \n",
        "from keras.layers import LeakyReLU\n",
        "from sklearn.datasets import make_blobs\n",
        "from tqdm.notebook import tqdm\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARSmh3SXotne"
      },
      "source": [
        "# Generate the synthetic dataset, already given!\n",
        "def get_real_data(n_samples):\n",
        "  data, _ = make_blobs(n_samples = n_samples, n_features = 2, centers = [(2,2), (-2,2), (-2,-2), (2,-2)], cluster_std=0.3)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcbgBGhd2iil"
      },
      "source": [
        "epochs = 1000\n",
        "batch_size = 512\n",
        "latent_dim = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOAkdA5A2nvm"
      },
      "source": [
        "def build_discriminator(dim):\n",
        "  model = Sequential()\n",
        "  # Activation = LeakyReLU(0.1)\n",
        "  #Your code here\n",
        " \n",
        "  model.compile('adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzTVn7kQ2tRA"
      },
      "source": [
        "def build_generator(latent_dim, output_dim):\n",
        "  model = Sequential()\n",
        "  # Activation = LeakyReLU(0.1)\n",
        "  #Your code here\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIJ-pm4D2utU"
      },
      "source": [
        "# Given a generator and a discriminator, build a GAN\n",
        "def build_GAN(G, D, latent_dim):\n",
        "  D.trainable = False\n",
        "  # Your code here\n",
        "\n",
        "  \n",
        "  GAN.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "  return GAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSj2pttV2w69"
      },
      "source": [
        "# Generate random uniform noise to input to the generator \n",
        "def generate_input_noise(batch_size, latent_dim):\n",
        "    return ____"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gii_nKoJ21Fd"
      },
      "source": [
        "# Build the GAN\n",
        "G = build_generator(latent_dim, 2)\n",
        "D = build_discriminator(2)\n",
        "GAN = build_GAN(G, D, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxZTIT6o22tf"
      },
      "source": [
        "# Training the GAN\n",
        "\n",
        "### Plotting stuff, do not touch ###\n",
        "x_min = -4; x_max = 4; y_min = -4; y_max = 4\n",
        "xx, yy = np.mgrid[(x_min):(x_max):.1, (y_min):(y_max):.1]\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "D_loss = []\n",
        "G_loss = []\n",
        "G_predict=[]\n",
        "D_contours = []\n",
        "### --------------------- ###\n",
        "\n",
        "for step in tqdm(range(epochs)):\n",
        "\n",
        "    # Training loop here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    # Collect training data to plot later. Do not touch\n",
        "    D_loss.append(_D_loss)\n",
        "    G_loss.append(_G_loss) \n",
        "\n",
        "    probs = D.predict(grid).reshape(xx.shape)\n",
        "    D_contours.append(probs)\n",
        "    test_noise = generate_input_noise(500, latent_dim)\n",
        "    fake_samples = G.predict(test_noise, batch_size=len(test_noise))\n",
        "    G_predict.append(fake_samples)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjsXHcKouHz"
      },
      "source": [
        "# See the training process\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14,6))\n",
        "plt.close(fig)\n",
        "ax1 = ax[0]\n",
        "ax2 = ax[1]\n",
        "\n",
        "def animate(i):\n",
        "  i = i*4\n",
        "  probs = D_contours[i]\n",
        "  probs[probs < 0.5] = 0\n",
        "  probs[probs >= 0.5] = 1\n",
        "  ax1.clear()\n",
        "  ax2.clear()\n",
        "  ax1.contourf(xx, yy, probs, 25, alpha = 0.4)\n",
        "  fake_data = G_predict[i]\n",
        "  real_data = get_real_data(500)\n",
        "  ax1.scatter(fake_data[:, 0], fake_data[:, 1], s = 10, label = 'Fake data')\n",
        "  ax1.scatter(real_data[:, 0], real_data[:, 1], s = 10, label = 'Real data')\n",
        "  ax1.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
        "  ax1.legend(loc=\"lower right\")\n",
        "  ax1.set_title(\"Epochs: {}\".format(i+1))\n",
        "  ax2.plot(np.arange(i), G_loss[0:i],label='G loss',c='darkred',zorder=50,alpha=0.8)\n",
        "  ax2.plot(np.arange(i), D_loss[0:i],label='D loss',c='darkblue',zorder=55,alpha=0.8)\n",
        "  ax2.set_xlim(-5, epochs+5)\n",
        "  ax2.legend()\n",
        "  ax2.set_xlabel('Epoch')\n",
        "\n",
        "anim = FuncAnimation(fig,animate,frames = epochs//4, interval=100, repeat = True)\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENfaIALmgdEA"
      },
      "source": [
        "## WGAN \n",
        "$$\n",
        "W(p_r,p_g) = \\frac{1}{K}\\sup_{‖f‖_L≤K} {E_{x∼p_r}[f(x)]−E_{x∼p_g}[f(x)]}\n",
        "$$\n",
        "For all $x_1, x_2 \\in \\mathbb{R}$:\n",
        "$$\n",
        "|f(x_1) - f(x_2)| \\leq K|x_1 - x_2|\n",
        "$$\n",
        "<br>\n",
        "Main changes:\n",
        "\n",
        "\n",
        "1.   Output of our critic is not restricted to $[0,1]$. It can take all **real values** and is interpreted as a **score** instead of a probability\n",
        "2.   Our Loss function is based on the **Wasserstein distance**\n",
        "3. We have to **clip our weights** in a range ($[-0.01, 0.01]$) to enforce Lipschitz continuity\n",
        "4. Update the critic more times than the generator\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "&\\mathcal{L}_{C} = -\\bigg(E_{x \\sim p_r}[C(x)] - E_{z \\sim p_z}[C(D(z))]\\bigg) \\\\\n",
        "&\\mathcal{L}_{G} = -E_{x \\sim p_g}[C(D(z))]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "To keep this coherent with the same convention that we have used, we may use labels as $x_{real} = +1$ and $x_{fake} = -1$ and then implement loss function as $mean(y_{true} \\times y_{pred})$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djo_7BFJtQ-f"
      },
      "source": [
        "epochs = 3000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeB9v2iLa0XT"
      },
      "source": [
        "def wasserstein(y_true, y_pred):\n",
        "    return ____"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePRdFP5YgaOD"
      },
      "source": [
        "# Build a discriminator neural network\n",
        "def build_critic(dim):\n",
        "\n",
        "  #Your code here\n",
        "\n",
        "  model.compile(Adam(learning_rate=0.002, beta_1=0.5),loss=wasserstein)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hkVL5bxbBti"
      },
      "source": [
        "def build_WGAN(G, C, latent_dim):\n",
        "\n",
        "  #Your code here\n",
        "  GAN.compile(Adam(learning_rate=0.005, beta_1=0.5),loss=wasserstein)\n",
        "  return GAN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDNrS2frbPQ8"
      },
      "source": [
        "G = build_generator(latent_dim, 2)\n",
        "C = build_critic(2)\n",
        "GAN = build_WGAN(G, C, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ldmE62nbV2X"
      },
      "source": [
        "#####\n",
        "n_critic = 3\n",
        "#####\n",
        "\n",
        "# Training the GAN\n",
        "C_loss = []\n",
        "G_loss = []\n",
        "G_predict=[]\n",
        "\n",
        "for step in tqdm(range(epochs)):\n",
        "\n",
        "\n",
        "    # Train Critic =====================================\n",
        "    for _ in range(n_critic):\n",
        "      # Train critic  \n",
        "\n",
        "\n",
        "      clip_threshold = 0.01      \n",
        "      # Clip weights in range [-0.01, 0.01]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Critic loop over ====================================\n",
        "\n",
        "    # Train the generator\n",
        "\n",
        "\n",
        "\n",
        "    # Collect training process for plotting later\n",
        "    C_loss.append(_C_loss)\n",
        "    G_loss.append(_G_loss) \n",
        "    test_noise = generate_input_noise(500, latent_dim)\n",
        "    fake_samples = G.predict(test_noise, batch_size=len(test_noise))\n",
        "    G_predict.append(fake_samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8KDlTkEdRSt"
      },
      "source": [
        "# See the training process\n",
        "fig, ax = plt.subplots(1, 2, figsize=(14,6))\n",
        "plt.close(fig)\n",
        "ax1 = ax[0]\n",
        "ax2 = ax[1]\n",
        "\n",
        "def animate(i):\n",
        "  i = i*10\n",
        "  ax1.clear()\n",
        "  ax2.clear()\n",
        "  fake_data = G_predict[i]\n",
        "  real_data = get_real_data(500)\n",
        "  ax1.scatter(fake_data[:, 0], fake_data[:, 1], s = 10)\n",
        "  ax1.scatter(real_data[:, 0], real_data[:, 1], s = 10)\n",
        "  ax1.set(xlim=(x_min, x_max), ylim=(y_min, y_max))\n",
        "  ax1.set_title(\"Epochs: {}\".format(i+1))\n",
        "  ax2.plot(np.arange(i), G_loss[0:i],label='G loss',c='darkred',zorder=50,alpha=0.8)\n",
        "  ax2.plot(np.arange(i), C_loss[0:i],label='D loss',c='darkblue',zorder=55,alpha=0.8)\n",
        "  ax2.legend()\n",
        "  ax2.set_xlabel('Epoch')\n",
        "\n",
        "anim = FuncAnimation(fig,animate,frames = epochs//10, interval=100, repeat = True)\n",
        "HTML(anim.to_html5_video())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10FjOSC6CnwI"
      },
      "source": [
        "### Readings:\n",
        "https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html"
      ]
    }
  ]
}